# Time Complexity, Part 3: Step Help

<div class="ai-author">Written by <b>Adil S. Iqbal</b>.</div><br />
<div class="ai-date">Published on <b>May 19, 2019</b>.</div><br />

<a href="/b2fd0303-9b4d-4b46-a4bf-ec64ca3eb906">**Continuation from Time Complexity, Part 2: Notation Station**</a>

Last time we talked about Time Complexity, we discussed some ways to describe the time complexity of certain operations. For example, when we say "constant time" we mean that the operation's runtime is not dependent on the size of the data-set. When we describe an operation as occurring in "linear time," we mean that the runtime of the operation is in some proportion to the size of the data.  When we say "quadratic time," the operation actually slows down as the data-set becomes large. 

How do we know whether our algorithm's runtime is 'constant,' or 'linear,' or 'quadratic?' To know that, we have to analyze our code. 

Unfortunately, <span class="ai-mark">there is no standardized procedure for conducting Big-O analysis</span>. This is due to the wonderfully varied flavors of algorithms/data structures. In other words, having a standard protocol for conducting Big-O analysis is much like having a standard protocol for herding cats. I can make some *suggestions*, but there is no guarantee that those suggestions will work in every situation. That said, I have tried to show how we can go about conducting Big-O analysis in a structured way; by providing steps. Here are the steps laid out concisely:

  1. Identify your Input Data-set(s).
  2. Assign Complexities to each Operation.
  3. Find the Most *Time Consuming* Route.
  4. Determine the Qualities of your *Worst Case* Input(s).
  5. Aggregate Relevant Individual Complexities.
  6. Drop All Constants.
  7. Drop All Non-Dominant Terms.

We will discuss each step individually and analyze this example algorithm as we go:

<div class="ai-euler">

##### Moving Average

The ```movingAverage``` function takes a parameter called ```inputArray```, which is an array of unsorted numbers and of unknown length. It also *returns* an array of numbers such that each number at index $i$ equals the arithmetic mean of all numbers from ```inputArray[0]``` to ```inputArray[i]``` (inclusive).

```javascript
function movingAverage(inputArray) {
  if (inputArray.length === 0) {
    return [];
  }
  const outputArray = [];
  for (let i = 0; i < inputArray.length; i += 1) {
    let sum = 0;
    for (let j = 0; j < i + 1; j += 1) {
      sum += inputArray[j];
    }
    const average = sum / (i + 1);
    outputArray.push(average);
  }
  return outputArray;
}
```

</div>

## Step 1: Identify your Input Data-set(s).

Recall from the previous article, that $n$ is a convention that defines the size of your data set *assuming* there is only one data-set. For our example algorithm, identifying the input data set is easy and obvious. We can easily assign $n$ to the length of the ```inputArray``` variable. What do we do for a more advanced senario? What happens if there is more than one data sets that you are processing? In that case, every data-set gets their own variable. 

Recall again, the problem posited by Cracking the Coding Interview. The same problem cited in the previous article:

<blockquote>

Suppose we had an algorithm that took in an array of strings, sorted each string, and then sorted the full array. What would the runtime be?

<div class="ai-author">- Cracking the Coding Interview, 6th Edition</div>

</blockquote>

Here is the answer provided:

<blockquote>

Let's define new terms-and use names that are logical.

* Let $s$ be the length of the longest string.
* Let $a$ be the length of the array.

Now we can work through this in parts:

* Sorting each string is $O(s\log s)$.
* We have to do this for every string (and there are $a$ strings), so that's $O(a\cdot s\log s)$.

Now we have to sort all the strings. There are $a$ strings, so you'll may be inclined to say that this takes $O(a\log a)$ time. This is what most candidates would say. You should also take into account that you need to compare the strings. Each string comparison takes $O(s)$ time. There are $O(a\log a)$ comparisons, therefore this will take $O(a\cdot s\log a)$ time.

If you add up these two parts, you get $O(a\cdot s(\log a +\log s))$.

This is it. There is no way to reduce it further.

<div class="ai-author">- Cracking the Coding Interview, 6th Edition</div>

</blockquote>

Again, don't worry if you don't quite understand all the Big-O notation or the entire procedure behind. Focus on the take-home message. When there is more than one data-set, <span class="ai-mark">choose variables that are descriptive and process them separately thereafter.</span>

This step is important because these <span class="ai-mark">variables remain consistent across the complexities of different algorithms designed for the same function.</span> We can then tell which algorithm is better just by comparing the Big-O notation of each algorithm. 

## Step 2: Assign Complexities to each Operation.

Each primitive operation should now be assigned a Big-O value individually. At this stage you may not know exactly how to go about doing this, but it will get easier with practice and the use of <a href="https://www.bigocheatsheet.com/" target="_blank">online resources</a>. Let's go line by line through the ```movingAverage``` function. 

* **Line 2:** Looking up an object property occurs in constant time. An equality comparison also occurs in constant time. This line executes in $O(1 + 1)$, which is $O(2)$.
* **Line 3:** Returning a value occurs in constant time. Defining a value (in this case, an empty array) also occurs in constant time. This line executes in $O(1 + 1)$, which is $O(2)$.
* **Line 5:** Declaring a variable and defining a value simultaneously occurs in constant time.  This line executes in $O(1)$.
* **Line 6:** During this line, we loop through our entire data-set once. <span class="ai-mark">Every operation that occurs inside this loop will be multiplied by $n$,</span> which is why the notation in the next sentence looks the way it does. This line executes in $O(n\times (\ldots ))$ time.
  * **Line 7:** Declaring a variable and defining a value simultaneously occurs in constant time.  This line executes in $O(1)$.
  * **Line 8:** During this line, we loop through *some fraction* of our data-set (which we're looping through on line #6). It's not the entire data-set, because the iteration of $j$ stops at $i$. Let's, for now, denote that fraction, whatever it is, as $f$. We'll continue our discussion of $f$ in Step #5.  That said, every operation that occurs inside this loop will be multiplied by $f$. This line executes in $O(f\times (\ldots ))$.
    * **Line 9:** When we refactor this line to read ```sum = sum + inputArray[i]```, we can see that there are three separate things going on here. First, an object property is being looked up (```inputArray[i]```). Second, an addition operation is occurring (```sum + inputArray[i]```). Third, the ```sum``` variable is being reassigned. All three of these operations occur in constant time. That is $O(1+1+1)$, which means this line executes in $O(3)$.
  * **Line 11:** There are two arithmetic operations, a simultaneous variable declaration and assignment occurring on this line. All three of these operations occur in constant time. That is $O(1+1+1)$, which means this line executes in $O(3)$.
  * **Line 12:** Inserting a value into an array occurs in $O(1)$ time. Notice that the array method's runtime is included our evaluation of the ```movingAverage``` function. ```(FYI: The online resource that was linked above says this operation occurs in linear time, though javascript implements a <a href="https://en.wikipedia.org/wiki/Dynamic_array#Performance" target="_blank">dynamic array</a> whose insert-at-end runtime is constant.)
* **Line 14:** Returning a value occurs in $O(1)$ time.

## Step 3: Find the Most *Time Consuming* Route

### What are we trying to accomplish?

When we conduct Big-O analysis we are talking about our algorithm's performance under a *worst case* scenario. What determines the worst-case scenario for our algorithm? <span class="ai-mark">The only thing we can change during our analysis is our input data. Therefore, the worst case scenario for our algorithm is that it receives *horrible* input data. The kind that makes our algorithm take the longest possible time to execute.</span>

The question that we are trying to answer is: **Exactly what are the qualities of our worst-case input?**

In the previous step, we evaluated the run-times of each individual operation in our algorithm. If we could now step through our algorithm again and trace the path that a hypothetical *worst case* input would take; where would that path lead?

### What is our overall strategy?

Our strategy will be to compile a list of conditions that, when met, will result in the execution of the most time consuming code blocks within our algorithm. Once we have a list, we can then work backwards to see if we can describe some qualities of this "worst case input."

Every situation is different, and you will have to use your intuition in certain scenarios. Here are some general tips when determining the most time consuming route:

### General Tips

#### Conditional Statements

When faced with the decision to route through one code block at the exclusion of another, choose the code block that takes the longest time to execute.

```javascript
function foo(data) {
  if (conditionA) {
    // code block that executes in constant time...
  } else if (conditionB) {
    // code block that executes in quadratic time... 
  } else {
    // code block that executes in linear time...
  }
}
```

In the above example, the most time consuming route would result if ```conditionA``` evaluated to ```false``` while ```conditionB``` evaluates to ```true```.

```javascript
function foo(data) {
  switch(someNumber) {
    case 1:
      // code block that executes in constant time...
      break;
    case 2:
      // code block that executes in linear time...
      break;
    case 3:
      // code block that executes in constant time...
      break;
    default:
      // code block that executes in constant time...
      break;
  }
}
```

In the above example, the most time consuming route would result if ```someNumber``` equals ```2```.

#### Dodge Early Exits & Optimizations

When using iteration or recursion, it's important to maximize the number of iterations and recursion calls. Recursive functions typically have an exit condition that should be avoided as much as possible (without hitting the stack limit).

```javascript
function foo(data) {
  if (conditionC) {
    return;
  }
  // ...
  for (let i = 0; i < data.length; i += 1) {
    if (conditionD) {
      return;
    }
    if (conditionE) {
      break;
    }
    if (conditionF) {
      continue;
    }
    // ...
  }
  // ...
}
```
The most time consuming route through the above code will result if ```conditionC``` evaluates to ```false``` and ```conditionD```, ```conditionE```, and ```conditionF``` evaluate to ```false``` for as many iterations of the for-loop as possible.

The ```movingAverage``` function has an optimization on Line #2 that should be avoided. It may seem obvious - but for the sake of the process, lets make a mental note that length of the ```inputArray``` parameter should be greater than ```0``` to avoid return statement on Line #3.

```javascript
function foo(data) {
  for (let i = 0; i < data.length; i += 1) {
    if (conditionG) {
      // code block that executes in constant time...
      continue;
    } else if (conditionH) {
      // code block that executes in constant time...
      break;
    } else {
      // code block that executes in constant time...
      return;
    }
  }
  // code block that executes in quadratic time...
}
```

The most time consuming route through the above code will result if ```conditionG``` evaluates to ```true``` for as many iterations as possible and we try to have ```conditionH``` evaluate to ```true``` for the last iteration. 

In the context of the above code, the take-home message is that to maximize the number of iterations, we prefer the ```continue``` keyword because it would stop the current iteration, but would allow the subsequent iterations to execute. We end on the ```break``` keyword, since it would terminate the loop, but would not stop the rest of the function from executing. The ```return``` keyword is to be avoided, since it would terminate the execution of the function.

#### Use Your Intuition

This list of general tips is by no means exhaustive. Nor could it be; when it comes to algorithms, the possibilities are endless. My hope is that the above examples helps guide your thinking process in novel situations.

## Step 4: Determine the Qualities of the *Worst-case* Input(s)

By now, you should have a list of conditions that a potential *worst-case* input needs to satisfy. Suffice it to say, there will be many scenarios where all of those conditions cannot be met; try to come up with a worst case input that satisfies as many as possible. If you're in the position of having to choose between satisfying two or more condition, choose the option with highest runtime.

In the case of the ```movingAverage``` function, the answer is that the worst-case input *must* have a length of at least 1. As we'll see in Step #6, the theoretical size of our input is more than satisfactory.

Though there are other cases where it's not at all clear cut and they require some measure of imagination. Allow me to offer an example; here's an animation of the search algorithm of a Binary Search Tree (also known as a "BST"):

<img class="img-responsive" src="https://media.giphy.com/media/VCyLLFXnhIBbIxeqkq/giphy.gif" alt="animation of search algorithm through binary search tree" />

The above animation shows that a BST can locate a value in 3 steps, whereas a typical array-like data structure would require 10 steps. In other words, the BST will, on *most* use cases, confer some runtime advantage compared to an array-like structure. However, not all BSTs are created equal; compare these two trees for instance:

<img class="img-responsive" src="https://i.imgur.com/OLMtmNh.png" alt="complete versus degenerate binary search trees" />

The top BST is what you would normally expect from a BST. Notice how the bottom BST arranges its values in a chained pattern. Running the same search algorithm on the bottom BST will confer no benefit in terms of Big-O run times and may even be *slower* in practice than an array. 

Even though, in **most** cases, the BST might be better for looking up values, its Big-O analysis is exactly the same as that of an array-like data structure, namely $O(n)$.

Take a look at the standard iterative search algorithm for BSTs and ask yourself the question "Could I have come up with the worst-case input based on the algorithm alone?"

```javascript
class Node {
  constructor(value) {
    this.value = value;
    this.left = null; // new Node would go here.
    this.right = null; // new Node would go here.
  }
}

function iterativeSearch(target, node) {
  while(node != null) {
    if (target > node.value) {
      node = node.right;
    } else if (target < node.value) {
      node = node.left;
    } else {
      return true;
    }
  }
  return false;
}
```

For those who answered "not likely," I assure you that you will get better at it. The more exposure you get to algorithms, data structures, and their associated study material; the more you will be able to maneuver problems like this. In fact, in a future article we're going to talk about a logarithmic time complexity that will shine quite a bit of light on algorithms like the one above.

### Limitations

The BST example should illustrate, that this method of determining the Big-O complexity of an algorithm is not a one-size-fits-all solution. You need to truly understand the algorithm you've designed, the kind of input you're receiving, and the environment your code is being executed in to really gauge the time complexity of an algorithm.

## Step 5: Aggregate Relevant Individual Complexities.

### Process the Worst-case Input

Once you've identified the qualities of the worst case input, run it through your algorithm while aggregating the individual complexities that are executed. If your worst-case input does not execute a block of code, the complexities of that code block should be skipped.

|Line|Runtime|Aggregate|
|:-:|:-:|:-:|
|2|$O(2)$|$O(2)$|
|3|**SKIP**|**SKIP**|
|5|$O(1)$|$O(3)$|
|6|$O(n\times (\ldots ))$|$O(n\times (\ldots ) + 3)$|
|7|$O(1)$|$O(n\times (1) + 3)$|
|8|$O(f\times (\ldots ))$|$O(n\times (f\times (\ldots ) + 1) + 3)$|
|9|$O(3)$|$O(n\times (f\times (3) + 1) + 3)$|
|11|$O(3)$|$O(n\times (f\times (3) + 4) + 3)$|
|12|$O(1)$|$O(n\times (f\times (3) + 5) + 3)$|
|14|$O(1)$|$O(n\times (f\times (3) + 5) + 4)$|

### Condensing the Final Aggregate

If we condense the final aggregate as is, we get:

##### $$O(3fn + 5n + 4)$$

<span class="ai-mark">In Big-O notation, all variables should represent data.</span> In the above equation, the value of $fn$ is a *"some fraction of our data."* We no longer have the luxury of being ambiguous.

#### The value of $fn$.

You may have noticed that when we assigned the notation for line #8, we said that it executes in $O(f\times (\ldots ))$ time. The following is a graph representing the behavior of this nested loop where the x-axis represents the value of $i$ (on line #6) and the y-axis represents the maximum value of $j$ (on line #8) for each $i$: 

<img class="img-responsive" src="https://i.imgur.com/42vBY7O.png" alt="a graph representing the behavior of c" style="{ max-width: 300px !important; }" />

The behavior of $j$ with respect to $i$ follows that of triangular numbers whose formula can be defined as:

##### $$ T_n= \sum_{k=1}^n k = 1+2+3+ \dotsb +n = \frac{n(n+1)}{2} $$

Further intuition about triangular numbers can be <a href="http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/runsums/triNbProof.html" target="_blank">found here</a>. If we set $fn$ equal to the above formula, we get:

##### $$ fn = \frac{n(n+1)}{2} $$

#### Completing the Math

##### $$O(3fn + 5n + 4) =$$

##### $$O(3\cdot \left(\dfrac{n(n+1)}{2}\right) + 5n + 4) =$$

##### $$O\left(\dfrac{3n^2+3n}{2} + \dfrac{10n}{2} + 4\right) =$$

##### $$O\left(\dfrac{3n^2+13n}{2} + 4\right) =$$

##### $$\boxed{O(1.5n^2 + 6.5n + 4)} $$


## Step 6: Drop All Constants

### What are "constants?"

In the context of Big-O analysis, the word "constant" could be referring to one of two things. It's important to distinguish between them:

1. A constant *factor* is a static multiplier of some base value.
2. A constant *term* is 0th order component of a polynomial equation. (More about that in Step #7)

In this step, we are specifically discussing constant *factors*. Let's look at an example. Here is a standard polynomial equation:

##### $$ax^2 + bx + c$$

Which can be expanded as:

##### $$ax^2 + bx^1 + cx^0$$

The constant *factors* in the above equation are $a$, $b$, and $c$. The constant *term* in the above equation is $cx^0$. In this step, we are specifically talking about the factors. Here is a more practical example with hard numbers: Let's look at the aggregated runtime of the ```movingAverage``` function from the previous section:

##### $$O(1.5n^2 + 6.5n + 4)=$$
##### $$O(1.5n^2 + 6.5n^1 + 4n^0)$$

The constant *factors* in the above equation are $1.5$, $6.5$, and $4$. The constant *term* in the above equation is $4n^0$. It might help for you to see this in another format; I'll name the individual terms below, their constant factor, and their base value.

|Name|Term|__Constant (factor)__|Base|
|:-:|:-:|:-:|:-:|
|Quadratic|$1.5n^2$|$1.5$|$n^2$|
|Linear|$6.5n^1$|$6.5$|$n^1=n$|
|__Constant (term)__|$4n^0$|$4$|$n^0=1$|

Please note that there is both a row and a column in the above table labelled "Constant," take the time to distinguish between them before moving on. In this step, we are only focused on constant *factors*.

### Chasing Infinity

Before we talk about why constants are bad, we should discuss $n$ for a moment. Recall this graph from the very first article:

<img class="img-responsive" src="https://i.imgur.com/38s0bke.png" alt="Time complexity graph" />

We said that we only care about how the algorithm performs after point $k$, because it helps us be more sure about our judgement. In the context of Big-O analysis, the value of point $k$ just keeps on getting bigger. Asking the question "How big does it get?" is missing the point. It just keeps getting bigger. It's said to be asymptotically close to infinity.

<span class="ai-mark">This means that $n$, which is the size of our worst-case input, also keeps getting bigger.</span>

### Why are constants bad?

<span class="ai-mark">The fact is that when sufficiently large data-sets are passed into our algorithms, it is not the constants, but the *type* of scaling that ultimately decides which runtime is worse.</span> In that context, constant factors represent unnecessary information that obfuscate the information that actually matters.

<span class="ai-mark">In Big-O analysis, we drop the constants so we can isolate the necessary information; that is, we want to isolate the *types* of scaling that comprise our runtime.</span>

### The Absurd Constant Experiment, Part I

We can test the theory that "it is not the constants, but the *type* of scaling that ultimately decides which runtime is worse." We can do this by comparing the runtimes of two functions over a very large data-set.

* **Function A** has a runtime of $1000000\times n$.
* **Function B** has a runtime of $n^2$.

These two functions now have to churn through a data-set with 1,200,000 items. If we graph their performance, we get:

<iframe src="https://www.desmos.com/calculator/whorcjsdln?embed" width="300px" height="300px" style="border: 1px solid #ccc" frameborder=0></iframe>

We see that even though **Function A** has a very large constant in tow, **Function B** will *still* overtake **Function A** over a large enough data-set. This would continue to hold true if **Function A** had a runtime of $n$ multiplied by any other constant factor.

### How do we get rid of constants?  

Our goal is to drop the constant factors from all the terms so that we're left with only the base values. To do that, we have to divide each term by its own constant factor:  

##### $$O\left(\dfrac{1.5n^2}{1.5} + \dfrac{6.5n}{6.5} + \dfrac{4}{4}\right) =$$
##### $$\boxed{O(n^2 + n + 1)} $$

## Step 7: Drop the Non-Dominant Terms.

### What is "dominance?"

Let's take a little time to get on the same page. What does the word "dominance" mean with respect to a mathematical term? <span class="ai-mark">When a term is said to have "dominance," it means that it is contributing the most to the overall value of that equation.</span> Let's look at this generic polynomial equation:

##### $$y = x^2 + x + 1$$

There are three terms in the above equation. The quadratic term: $x^2$; the linear term: $x$; and the constant term $1$. Which of these terms contributes the most to the value of $y$ at $x=3$? Let's look at the graph:

<iframe src="https://www.desmos.com/calculator/wmgiwku1bn?embed" width="300px" height="300px" style="border: 1px solid #ccc" frameborder=0></iframe>

The quadratic term contributes the most, followed by the linear term, and finally the constant term contributes the least. The quadratic term is therefore the dominant term at $x=3$. It follows that both the linear and constant terms are non-dominant terms at $x=3$.

#### "Lower Order" and "Higher Order" Terms

When studying Big-O notation, you will come across mathematical terms referred to as "lower order" and "higher order." Many other tutorials confuse these phrases with "non-dominant" and "dominant" respectively. That is a huge mistake.

The phrases "lower order" and "higher order" refer to mathematical terms in a polynomial equation. Take the polynomial above:

##### $$y = x^2 + x + 1$$

The order of each term is determined by the exponent of $x$. The quadratic term is a 2nd order term. The linear term is a 1st order term; the exponent of $x$ in that case is 1 and anything raised to the power of 1 equals itself. The constant term is a 0th order term; the exponent of $x$ in that case is 0 and anything raised to the power of 0 equals 1.

The confusion arises because often in Big-O analysis, the highest order polynomial term is also the dominant term. Though, that is not always the case! In future articles, we will discuss types of scaling that are far more dominant than high order polynomials, such as $O(n!)$, $O(2^n)$, and $O(\infty )$. 

### Why do we drop non-dominant terms? 

<span class="ai-mark">Dominant terms tend to become more dominant as $x$ gets larger.</span> Let's look at contributions of the terms of the equation $y = x^2 + x + 1$ at $x=5$.

<iframe src="https://www.desmos.com/calculator/uu0o5gjkar?embed" width="300px" height="300px" style="border: 1px solid #ccc" frameborder=0></iframe>

Notice that the contribution percentages are different than they were at $x=3$. Let's quantify those differences:

|Term|$x=3$|$x=5$|Difference|
|:-:|:-:|:-:|:-:|
|Quadratic|69.2%|80.7%|+11.5%|
|Linear|23.1%|16.1%|-7.0%|
|Constant|7.7%|3.2%|-4.5%|

You should notice that the contribution from the quadratic term has grown, while the contribution from the non-dominant terms have shrunk. This trend continues as $x$ becomes larger and larger, until eventually the dominant term comprises >99% of the value of $y$. Once $x$ becomes large enough, we can ignore the non-dominant terms and still arrive at an acceptable approximation of $y$.

### The Absurd Constant Experiment, Part II 

Let's test the theory that "\[once the amount of data\] becomes large enough, we can ignore the non-dominant terms and still arrive at an acceptable approximation of \[the runtime\]." What would happen if we repeated the Absurd Constant Experiment while dropping the constants on the two functions (the same way we did to the ```movingAverage``` function)?

* **Function A** now has a runtime of $n$.
* **Function B** now has a runtime of $n^2$.

These two functions still have to churn through a data-set with 1,200,000 items. If we graph their performance, we get:

<iframe src="https://www.desmos.com/calculator/vc8kzyhchn?embed" width="300px" height="300px" style="border: 1px solid #ccc" frameborder=0></iframe>

Notice that, without a constant, **Function A** is so tiny in comparison to **Function B**, that its barely even visible on the graph. We can conclude from these results that non-dominant terms have a miniscule value compared to dominant terms over very large data sets.

The point I'm trying to make is that non-dominant terms cannot possibly determine the outcome of an algorithm's worst-case runtime when the size of the data set is asymptotically close to infinity, *especially* when stripped of their constants (both of these things are true in Big-O analysis). <span class="ai-mark">We drop non-dominant terms because their contribution is so miniscule/negligable that including them would needlessly complicate any straight forward comparison of the runtimes of two or more algorithms.</span>

### Application

We have, so far, identified the worst-case runtime of the ```movingAverage``` function and isolated the *types* of scaling that comprise its runtime (by dropping the constants). Let's now complete our analysis be removing the non-dominant terms:

##### $$O(n^2 + \not{n} + \not{1})$$
##### $$\boxed{O(n^2)} $$

## Wrapping Up

The ```movingAverage``` function has a Big-O complexity of $O(n^2)$. Let's summarize what we've talked about:

* There is no standardized procedure for conducting Big-O analysis.
* These are steps will help guide your thinking:
  * **Step #1: Identify your Input Data-set(s).**
    * Choose variables that are descriptive and process them separately thereafter.
    * Variables remain consistent across the complexities of different algorithms designed for the same function.
  * **Step #2: Assign Complexities to each Operation.**
    * Every operation that occurs inside a loop over your data-set will be multiplied by $n$ (or whichever variable was assigned to that data-set).
    * The complexity of a method or function executed within an algorithm is included in the complexity that algorithm.
  * **Step #3: Find the Most *Time Consuming* Route.**
    * The worst case scenario for your algorithm is if it recieves horrible input data.
    * Maintain list of conditions that need to be met to execute most time consuming code blocks.
    * Maintain list of conditions that maximize iterations and avoid early exits and optimization.
  * **Step #4: Determine the Qualities of your *Worst Case* Input(s).**
    * Brainstorm a worst case input that meets as many conditions on your list of conditions as possible.
    * Requires complete understanding of algorithm, input data structure, and runtime enviornment.
  * **Step #5: Aggregate Relevant Individual Complexities.**
    * All variables in final aggregate should represent some aspect of your input data.
  * **Step #6: Drop All Constants.**
    * We assume that the size of our input data is asymtotically close to infinity.
    * When data size is large enough, it's not constants but scaling type that determines worst-case runtime.
    * We drop constants to isolate scalings that comprise our worst-case runtime.
    * Drop constants by dividing each term by its own constant.
  * **Step #7: Drop All Non-Dominant Terms.**
    * When a term is "dominant," it is contributing the most to the overall value of the equation.
    * Dominant terms tend to increase their share of the contribution as the dataset increases.
    * If the data-set is large enough, dominant terms will eventually comprise > 99% of the value of the equation.
    * We drop non-dominant terms because their contribution so miniscule/negligable that including them would needlessly complicate any straight forward comparison between two algorithms.

So far in our series, I've tried to provide simple and obvious examples/analogies for everything we've talked about.  That said, there is a lot of nuance hidden between the words in these articles. That is why, next time, we're going to do lots of practice problems to round out our understanding. Since I find it hard to publish an article without any written content, we'll also talk about logarithmic time complexity. 