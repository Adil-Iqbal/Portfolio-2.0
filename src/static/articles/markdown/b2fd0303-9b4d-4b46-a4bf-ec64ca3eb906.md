# Time Complexity, Part 2: Notation Station

<div class="ai-author">Written by <b>Adil S. Iqbal</b>.</div><br />
<div class="ai-date">Published on <b>June 6, 2019</b>.</div><br />

<a href="/d37c9394-eddb-446e-95a7-66ac15112dd1">**Continuation from Time Complexity, Part 1: Intro to Big-O**</a>

Last time we spoke about Time Complexity, we broached the topic of Big-O analysis. We said that Big-O analysis helps us judge how our algorithms' speed is effected as data-sets become large. We also implied that Big-O analysis is done assuming the least optimal conditions possible for our algorithms. In upcoming articles, we will conduct proper Big-O analysis, but first we need the 'vocabulary' necessary to talk about our code.

In the same way that a Ferrari on a long stretch of open road will perform better than a beat up ice cream truck: some algorithms' speed will scale fabulously and others poorly as our data-sets get larger. This article is about how we can express those differences.

## 'Constant Time' also known as $O(1)$

Writing an algorithm that scales in <span class="ai-mark">"constant time"</span> is like inventing a car that can travel around the *earth *in the same time as it takes to travel down the *block*. It <span class="ai-mark">implies that your algorithm doesn't slow down at all, regardless of the data size.</span> In most programming languages, arithmetic operators, inequalities, and most hash-map operations (when keys are already known) occur in constant time. If you have solved a technical interview question with an algorithm that scales in constant time, chances are that you're in great shape. Consider that ```let x = 0; x += 5;``` executes at the same speed as ```let y = 0; y += 1000000;``` even though the integer ```1000000``` requires more storage space than the number ```5```. If we were to graph this behavior, it would look like this:

<iframe frameborder="0" height="400px" src="https://www.desmos.com/calculator/24vi7lp4kl?embed" style="border: 1px solid #ccc;" width="400px"></iframe>

## 'Linear Time' also known as $O(n)$

### Brief, but necessary, tangent.

The first time I saw the term $O(n)$, the knee-jerk question that came to mind was, "What does $n$ mean?" <span class="ai-mark">The variable $n$ is a convention. It assumes your algorithm is processing *one* data-set and refers to the size of that data-set.</span><span class="ai-mark"> The notation $O(\ldots )$ refers to how the runtime of your algorithm will scale.</span> In my car analogies, $n$ refers to the *distance* the car has to travel while $O(\ldots )$ describes the *time* required to reach the destination.

In the real world, it's a rookie mistake to only think in terms of $n$, since there may be multiple data-sets that your algorithm is processing. To illustrate that point, here is an excerpt from Cracking the Coding Interview, 6th Edition:


<blockquote>

Suppose we had an algorithm that took in an array of strings, sorted each string, and then sorted the full array.

What would the runtime be? Many candidates will reason the following: sorting each string is $O(n \log n)$ and we have to do this for each string, so that's $O(n\cdot n \log n)$. We also have to sort this array, so that's an additional $O(n \log n)$ work. Therefore,the total runtime is $O(n^2 \log n + n \log n)$, which is just $O(n^2 \log n)$.

This is completely incorrect. Did you catch the error?

The problem is that we used $n$ in two different ways. In one case, it's the length of the string (which string?). And in another case, it's the length of the array.

<div class="ai-author">- Cracking the Coding Interview, 6th Edition</div>

</blockquote>

Don't worry if you didn't understand all the notation that was included in the excerpt, just focus on the take home message. <span class="ai-mark">In Big-O notation, different data-sets get their own variable.


From this point onward, I'll be using a code examples to 
illustrate my point. While not strictly required, it may be useful for 
you to type out the code for yourself and execute it. The concepts that
 I'm discussing can be generalized to any programming language; I will 
be using Javascript with Node. I encourage you to use this <a href="https://repl.it/languages/Nodejs" target="_blank">online Node Editor</a> to follow along.

### Back to 'Linear Time'

Writing an algorithm that operates in linear time is like inventing a Toyota Carolla. The more road between you and your destination, the longer your commute (in proportion to the distance). Similarly, <span class="ai-mark">in a linear time algorithm, if the data-set gets bigger, the runtime is increased by a steady amount (in proportion to $n$). Your algorithms will scale in linear time if they contain a loop of $n$ cycles optionally with respect to an iterable data-type containing $n$ items. This is because we have to "touch" each item in the data-set at least once</span>. The last few sentences were a mouthful! To illustrate them, consider this code:

```javascript
const data = [1, 2, 3];
const size = data.length;

let count = 0;

for (let i = 0; i < size; i++) {
  count = count + 1;
}

console.log(size, count);
```

* In the above code, <code class="lang-javascript">size</code> represents the size of the data-set, which is the same as $n$.
* The <code class="lang-javascript">count</code> variable represents the number of operations that are conducted. Since the operation we are performing occurs in constant time and we are performing the same operation with each iteration, we can use the <code class="lang-javascript">count</code> variable to approximate the behavior of $O(\ldots )$, which describes the runtime of our code.
* In comparing the <code class="lang-javascript">size</code> variable to the <code class="lang-javascript">count</code> variable, we are actually comparing how our data size is effecting our runtime. Let's execute our code to see what it prints:

```shell
3 3
```

We can see that the ```count``` variable matches the ```size``` variable! If you add/remove items to the ```data``` array, you will see that the ```count``` variable will continue to match the ```size``` variable so long as the rest of the code remains unaltered. In other words, our runtime is proportional to the size of our data. Since we are working with $n$ items, we can say that our runtime is $O(n)$. If we were going to graph this behavior, it would look something like this:

<iframe frameborder="0" height="400px" src="https://www.desmos.com/calculator/n4ncohdxre?embed" style="border: 1px solid #ccc;" width="400px"></iframe>

### Addition in Linear Time

If you've spent any time coding, you've probably had to loop through your data multiple times. Let's consider the following code:

```javascript
const data = [1, 2, 3];
const size = data.length;

let count = 0;

for (let i = 0; i < size; i++) {
  count = count + 1;
}

for (let i = 0; i < size; i++) {
  count = count + 1;
}

console.log(size, count);
```

Before we even run this code, we should be able to guess the results. How would we express the time complexity of that code in Big-O notation? Well, we looped through our entire data-set twice, so it would be $O(n) + O(n) = O(2n)$. Let's run the code and see if we're correct:

```shell
3 6
```

Correct! Since we're looping through the data twice, we are having to do twice as many operations.  That is why the <code class="lang-javascript">count</code> variable is now double the value of <code class="lang-javascript">size</code>. In the real world, the ability to optimize an algorithm from $O(2n)$ to $O(n)$ is likely a valuable optimization. <span class="ai-mark"> That said, in Big-O analysis, we typically don't care about the difference between $O(2n)$ and $O(n)$ because the *type* of scaling is still linear.</span> If we were to graph the runtime of the above code as it scales with larger data-sets, it would look like this:

<iframe frameborder="0" height="400px" src="https://www.desmos.com/calculator/6lwxwyaud5?embed" style="border: 1px solid #ccc;" width="400px"></iframe>

In the next article, I'll discuss why Big-O analysis emphasizes the *type * of scaling instead of the *actual* scaling. For now, we need to talk about one more operation.

### Multiplication in Linear Time.

We have seen addition, but what about multiplication? Ask yourself, "What is the relationship between addition and multiplication?" When we say $3\times 3$, what do we mean? Well, that's $3 + 3 + 3$.  We are essentially looping over the "plus 3" operation 3 times.  When we multiply $O(n)\times O(n)$, we're looping over your data to perform some operation "data" times. Consider this code:

```javascript
const data = [1, 2, 3];
const size = data.length;

let count = 0;

for (let i = 0; i < size; i++) {
  for (let j = 0; j < size; j++) {
    count = count + 1;
  }
}

console.log(size, count);
```

It's very important to contrast the multiplication code with the addition code. In both instances, there are two loops. However, the <span class="ai-mark">multiplication code contains a loop within a loop; also called a "nested loop."</span> This is a very common point of confusion and its important to distinguish the two because linear multiplication results in another *type* of scaling. When we execute the code above, this is printed:

```shell
3 9
```

This result may not look too bad but try adding a few more items to the ```data``` array. As you add more and more items, you'll notice that the ```count``` variable is increasing much more than the ```size``` variable. If you were to graph the relationship, it would look like this:


<iframe frameborder="0" height="400px" src="https://www.desmos.com/calculator/zvmwl4wsud?embed" style="border: 1px solid #ccc;" width="400px"></iframe>

## 'Quadratic Time' also known as $O(n^2)$

If you've written an algorithm in quadratic time, you've just invented a car that *slows down* as the road becomes longer. This means that <span class="ai-mark">as your data-set gets larger, your algorithm will have to perform exponentially more operations. Quadratic time complexity results when your code contains a nested loop.</span> All of this should be ringing a bell. We discussed this in the "Multiplication in Linear Time" section above. After all, $O(n)\times O(n) = O(n^2)$.

## Wrapping Up

Let's summarize what we've talked about:

* $n$ means the size of the data that your processing (assuming only one data-set).
* $O(\ldots )$ describes the worst-case runtime of your algorithm.
* 'Constant time' or $O(1)$ means that your algorithm's speed is independent of your data-size.
* 'Linear time' or $O(n)$ means that your algorithm's speed is proportional to your data-size.
  * Linear addition occurs when you loop through your data-set sequentially.
  * Linear multiplication occurs when you loop through your data within a loop through your data - or a nested loop.
* 'Quadratic time' or $O(n^2)$ means that your algorithm's speed slows down as your data-set gets larger. It is the result of linear multiplication.

Now that we have *some* vocabulary under our belt, in the next article we're going discuss the "rules" of Big-O analysis and why they exist. We'll also analyze our first bit of code using Big-O analysis.

<a href="/299acdd6-6474-4e91-a6b9-d5cfa37c5ab6">**Next Article: Time Complexity, Part 3: Step Help**</a>

**P.S.** - You may be wondering about other *types* of scalings, such $O(\log n)$, $O(n\log n)$, $O(2^n)$, $O(n!)$, or $O(\infty )$. Those will all be covered in a future article. Also, I'm well aware that you don't necessarily have to "loop" through your data to achieve an $O(n)$ complexity, but it's a good place to begin learning the concept. In future articles, I'll be going through tons of examples which will hopefully add a more nuanced understanding to the concepts I've described here.